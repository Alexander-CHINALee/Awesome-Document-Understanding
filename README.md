# Awesome-Document-Understanding [![Awesome](https://awesome.re/badge-flat.svg)](https://awesome.re)
> A repository used to collect various document artifical intelligence

continue update ðŸ¤—

## Table of contents

- [Document Understanding](#document-understanding)
- [Video LLM](#video-llm)

# Document Understanding

<details open>
<summary>2023</summary>

- [mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model](https://arxiv.org/abs/2311.18248) **(Alibaba)** | 23.11.30  | arXiv | [Code](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main)
- [Towards Improving Document Understanding: An Exploration on Text-Grounding via MLLMs](https://arxiv.org/abs/2311.13194) **(USTC)** | 23.11.22 | arXiv
- [DocPedia: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding](https://arxiv.org/abs/2311.11810) **(USTC,ByteDance)** | 23.11.20 | arXiv
- [Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models](https://arxiv.org/abs/2311.06607) **(HUST)** | 23.11.11 | arXiv | [Code](https://github.com/Yuliang-Liu/Monkey)
- [Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation](https://arxiv.org/abs/2310.16809) **(SCUT)** | 23.10.25 | arXiv | [Code](https://github.com/SCUT-DLVCLab/GPT-4V_OCR)
- [UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model](https://arxiv.org/abs/2310.05126) **(DAMO,RUC,ECNU)** | 23.10.08 | arXiv | [Code](https://github.com/LukeForeverYoung/UReader)
- [Kosmos-2.5: A Multimodal Literate Model](https://arxiv.org/abs/2309.11419) **(MSRA)** | 23.9.20 | arXiv | [Code](https://thegenerality.com/agi/)
- [UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding](https://arxiv.org/abs/2308.11592) **(USTC,ByteDance)** | 23.8.19 | arXiv
- [mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding](https://arxiv.org/abs/2307.02499) **(DAMO)** | 23.7.4 | arXiv | [Code](https://github.com/X-PLUG/mPLUG-DocOwl)
- [On the Hidden Mystery of OCR in Large Multimodal Models](https://arxiv.org/abs/2305.07895) **(HUST,SCUT,Microsoft)** | 23.5.13 | arXiv | [Code](https://github.com/Yuliang-Liu/MultimodalOCR)
- [Visual Information Extraction in the Wild: Practical Dataset and End-to-end Solution](https://arxiv.org/abs/2305.07498) **(HUST)** | 23.5.12 | arXiv | [Code](https://github.com/jfkuang/CFAM)
- [Document Understanding Dataset and Evaluation (DUDE)](https://arxiv.org/abs/2305.08455) | 23.5.15 | arXiv | [Website](https://rrc.cvc.uab.es/?ch=23)
- [StrucTexTv2: Masked Visual-Textual Prediction for Document Image Pre-training](https://arxiv.org/abs/2303.00289) **(Baidu)** | 23.03.01 | ICLR23 | [Code](https://github.com/PaddlePaddle/VIMER/tree/main/StrucTexT)

</details>

<details open>
<summary>2022</summary>

- [Wukong-Reader: Multi-modal Pre-training for Fine-grained Visual Document Understanding](https://arxiv.org/abs/2212.09621) **(Huawei)** | 22.12.19 | ACL23
- [Unifying Vision, Text, and Layout for Universal Document Processing](https://arxiv.org/abs/2212.02623) **(Microsoft)** | 22.12.05 | CVPR23 | [Code](https://github.com/microsoft/i-Code/tree/main/i-Code-Doc)
- [XYLayoutLM: Towards Layout-Aware Multimodal Networks For Visually-Rich Document Understanding](https://arxiv.org/abs/2203.06947) **(Alibaba)** | 22.3.14 | [Code Unofficial](https://github.com/Sanster/xy-cut)
- [LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking](https://arxiv.org/abs/2204.08387) **(Microsoft)** | 22.04.18 | ACM MM22 | [Code](https://github.com/microsoft/unilm/tree/master/layoutlmv3)
- [DiT: Self-supervised Pre-training for Document Image Transformer](https://arxiv.org/abs/2203.02378) **(Microsoft)** | 22.03.04 | ACM MM22 | [Code](https://github.com/microsoft/unilm/tree/master/dit)
- [ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding](https://arxiv.org/abs/2210.06155) **(Baidu)** | 22.10.12 | arXiv | [Code](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/ernie-layout)
- [Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark](https://arxiv.org/abs/2202.06767) **(Huawei)** | 22.2.14 | NIPS22 | [Code](https://wukong-dataset.github.io/wukong-dataset/)
- [Unified Pretraining Framework for Document Understanding](https://arxiv.org/abs/2204.10939) **(Adobe)** | 22.04.22 | NIPS21

</details>


<details open>
<summary>2021</summary>

- [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836) **(Microsoft)** | 21.04.18 | arXiv | [Code](https://github.com/microsoft/unilm/tree/master/layoutxlm)
- [LayoutReader: Pre-training of Text and Layout for Reading Order Detection](https://arxiv.org/abs/2108.11591) **(Microsoft)** | 21.08.26 | EMNLP21 | [Code](https://github.com/microsoft/unilm/tree/master/layoutreader)
- [Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer](https://arxiv.org/abs/2102.09550) **(Applica)** | 21.02.18 | ICDAR21 | [Code](https://github.com/uakarsh/TiLT-Implementation)

</details>


<details open>
<summary>2020</summary>

- [LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding](https://arxiv.org/abs/2012.14740) **(Microsoft)** | 20.12.29 | arXiv | [Code](https://github.com/microsoft/unilm/tree/master/layoutlmv2)

</details>

<details open>
<summary>2020</summary>
  
- [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://arxiv.org/abs/1912.13318) **(Microsoft)** | 19.12.31 | KDD20 | [Code](https://github.com/microsoft/unilm/tree/master/layoutlm)

</details>


# Video LLM

<details open>
<summary>2023</summary>

- [TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding](https://arxiv.org/abs/2312.02051) **(PKU,Noah)** | 23.12.04  | arXiv | [Code](https://github.com/RenShuhuai-Andy/TimeChat)
- [Video-LLaVA: Learning United Visual Representation by Alignment Before Projection](https://arxiv.org/abs/2311.10122) **(PKU,PengCheng)** | 23.11.16 | arXiv | [code](https://github.com/PKU-YuanGroup/Video-LLaVA)
- [Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding](https://arxiv.org/abs/2311.08046) **(PKU,PengCheng)** | 23.11.14  | arXiv | [Code](https://github.com/PKU-YuanGroup/Chat-UniVi)
- [Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding](https://arxiv.org/abs/2306.02858) **(DAMO)** | 23.06.05 | arXiv | [code](https://github.com/DAMO-NLP-SG/Video-LLaMA)

</details>

